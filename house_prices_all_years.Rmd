---
title: "Using Land Registry Data to Estimate Market Value of Property by Area"
output:
  html_notebook: default
  pdf_document: default
---

This analysis uses the publicly available data from the Land Registry to 
estimate a reasonable market value for each property type in each area. The 
conclusion is that the data are sufficient to set fairly broad, but useful, 
boundaries for transactions at 'fair market value' in most areas.

## Data Preparation

To begin with we load the libraries required for the analysis.

```{r, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(data.table)
library(readr)
library(purrr)
library(lubridate)
library(stringr)
```

Now we create a function that will check whether the price paid data are present 
locally. If so it will read that into memory as `full_data`. If not it will 
download the price paid data and create the `full_data` object.
```{r}
create_full_data_all_years <- function() {
    make_url <- function(year) {
        paste0("http://prod2.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-",
               year,
               ".txt")
    }
    
    years <- seq(1995, 2017)
    
    urls <- map_chr(years, make_url)
    if(!dir.exists("data/")) {
        dir.create("data/")
    }
    setwd("data")
    walk(urls, function(x) {
        if(!file.exists(basename(x))) {
            download.file(x, basename(x), method = "curl")
        }
    })
    
    make_filename <- function(year) {
        paste("pp-",
              year,
              ".txt",
              sep = "")
    }
    
    filenames <- basename(urls)
    
    datalist <- map(filenames, 
                    function(x) fread(x,
                                      sep = ",",
                                      header = F, 
                                      col.names = c('tuid', 'price', 
                                                    'date_of_transfer', 
                                                    'postcde', 'prop_typ', 
                                                    'old_new', 'duration', 
                                                    'paon', 'saon', 'street', 
                                                    'locality', 'town', 
                                                    'district', 'county', 
                                                    'ppd_type', 
                                                    'rec_status')))
    
    full_data <- rbindlist(datalist)
    rm(datalist)
    setwd("..")
    # datalist <- lapply(list.files(pattern = "pp-\\d{4}\\.txt"), fread, sep = ",")
    
    full_data[
        , 
        prop_typ := as.factor(prop_typ)
        ][
            , 
            c("outcde", "incde") := tstrsplit(postcde, " ", fixed = TRUE)
            ][
                , 
                date_of_transfer := as_date(date_of_transfer)
                ][
                    , 
                    year := year(date_of_transfer)
                    ][
                        , 
                        tax_year := ifelse(month(date_of_transfer) >= 4 & day(date_of_transfer) >= 6, 
                                           year + 1,
                                           year)
                        ]
    
    saveRDS(full_data, './data/full_data_all_years.rds')
}
```

The next step is to run that function, checking first that the required object 
is not already in memory.

NB. These extra steps are to prevent unnecessary downloading of the data or 
creation of the data objects, as they are rather large.

```{r, results = 'hide'}
if(!exists('full_data')) {
    ifelse(file.exists('data/full_data_all_years.rds'),
           invisible(full_data <- as.data.table(readRDS('data/full_data_all_years.rds'))),
           create_full_data())
}
```

## Data Munging

Now apply grouping and summarising functions to the data to get what we need: 
entries for each combination of year, property type, and postcode area/outcode 
(e.g. M33, UB6).

```{r, results = 'hide'}
# Group the data by outcode, year, and property type, then summarise with a few 
# key stats
if(!exists('by_outcde_yr_typ')) {
    ifelse(file.exists('data/by_outcde_yr_typ_all_yrs.rds'),
           by_outcde_yr_typ <- as.data.table(readRDS('data/by_outcde_yr_typ_all_yrs.rds')),
           by_outcde_yr_typ <- full_data_all_years[, 
                                                   .(.N, 
                                                     avg_price = mean(price),
                                                     median = quantile(price, .50),
                                                     sd = sd(price),
                                                     q25 = quantile(price, .25),
                                                     q75 = quantile(price, .75)),
                                                   keyby = .(outcde, prop_typ, tax_year)])
}
```

Save the summarised table to file to speed up the process when re-running.

```{r}
# Write out the data.table to rds
if(!file.exists('data/by_outcde_yr_typ_all_yrs.rds')) {
    write_rds(by_outcde_yr_typ, 'data/by_outcde_yr_typ_all_yrs.rds')
}
```

Run a quick check for data quality.

```{r}
# Check how many groups have at least 10 data points
paste0(round((100 * nrow(by_outcde_yr_typ[N > 20]) / nrow(by_outcde_yr_typ))),
       "% of outcode, year, property type groups have at least 10 data points.")
```

The next step is to compare our list to the reference list of postcode areas, 
to ensure that we have entries for every possible combination (since any groups 
with no transactions will be missing from our list).

In this analysis the postcode lists have already been downloaded from the 
Ordnance Survey in `.csv` format, and stored in a directory called `OS_data`. 

```{r}
# Combine the Ordnance Survey postcode list csv files to get full list of all 
# UK outcodes.
fils <- list.files('./data/OS_data', 
                   pattern = '.csv$', 
                   full.names = TRUE)

pcdes <- rbindlist(lapply(fils, 
                          fread))[, 
                                  .(pcde = paste(str_extract(V1, 
                                                             '^[A-z]{1,2}\\d{1,2}[A-z]?'),
                                                 str_extract(V1, 
                                                             '\\d[A-z]{2}$')),
                                    outcde = str_extract(V1, 
                                                         '^[A-z]{1,2}\\d{1,2}[A-z]?'),
                                    incde = str_extract(V1, 
                                                        '\\d[A-z]{2}$'))]
```

Now test that the postcodes all fit the required format.

```{r}
# Test whether all pcdes have required format
min(str_detect(pcdes$pcde, "[A-z]{1,2}\\d{1,2}[A-z]? \\d[A-z]{2}")) == 1
```

```{r}
# Test whether all outcdes have required format
min(str_detect(pcdes$outcde, "[A-z]{1,2}\\d{1,2}[A-z]?")) == 1
```

We only need the outcodes, so we can reduce the size of the object dramatically. 
Importantly we add a row for a blank string, to match to those sales in the 
price paid data with no geographical location.

Then we cross join that table with all possible years and property types.


```{r}
# Drop cols and duplicates from pcdes to save memory
outcdes <- rbindlist(list(list(''), 
                          unique(pcdes[, .(outcde)])))

# Prepare all_outcdes for binding with year and type data
outcdes_yrs_typs <- CJ(outcde = outcdes$outcde, 
                       tax_year = 1995:2018,
                       prop_typ = c('D', 'F', 'O', 'S', 'T'))

```

We now join the outcodes to the price paid data, and add a logical column to 
flag Scottish postcodes. Due to the separation of Scottish property taxes we 
will remove those areas from the analysis.

The two `sapply()` operations at the end of this section test for missing 
values in the combined dataset.

```{r}
# Left Join full list of outcodes with the price paid data, see which have no 
# data. Add logical for Scottish pcdes.
scot_area_cdes <- c('AB', 'DD', 'DG', 'EH', 'FK', 'G', 'HS', 'IV',
                    'KA', 'KW', 'KY', 'ML', 'PA', 'PH', 'TD', 'ZE')
scot_outcde_regx <- paste0('^',
                           scot_area_cdes,
                           '[0-9]{1,2}',
                           collapse = '|')
setkey(outcdes_yrs_typs, outcde, prop_typ, tax_year)
setkey(by_outcde_yr_typ, outcde, prop_typ, tax_year)
all_ppdata <- by_outcde_yr_typ[outcdes_yrs_typs][
    , 
    scottish := (grepl(scot_outcde_regx, outcde))
]

sapply(all_ppdata, function(x) {max(is.na(x))})
sapply(all_ppdata, function(x) {sum(is.na(x))})
```

Now we separate the Scottish data, leaving an object `main_ppdata` that we can 
use for analysis. We also impute zeroes for the `NA` entries in the `N` field, 
which will make our analysis easier.

```{r}
# Separate Scottish data
main_ppdata <- all_ppdata[scottish == FALSE]
main_ppdata$N[is.na(main_ppdata$N)] <- 0L
scot_ppdata <- all_ppdata[scottish == TRUE]
```


## Analysis

### Data Quantity

An important question to answer is: for which outcode-year-type groups do we 
have sufficient data to be able to draw reliable inferences?

We can visualise this to begin with.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(ggthemes)
library(hrbrthemes)
extrafont::loadfonts()
ggplot(main_ppdata, aes(N)) +
    geom_histogram(binwidth = 30, fill = "#008985") +
    theme_ipsum_rc() +
    ggtitle("Number of Observations in Each Group") +
    labs(x = "N", y = "Count of Groups") +
    scale_y_comma() +
    scale_x_comma()
```

This doesn't look promising, but there is a long tail here that may not be 
captured clearly in the histogram.

```{r}
main_ppdata[, 
            .N, 
            keyby = .(low_data = (N < 10 & !is.na(N)),
                      no_data = (N == 0))
            ]
```

So of the `r nrow(main_ppdata)` groups in our dataset, 
`r nrow(main_ppdata[N < 10])` have 1-9 observations; `r sum(is.na(main_ppdata$N))` 
have none. That means `r nrow(main_ppdata[N >= 10])` of the groups have at 
least 10 observations, or 
`r round(100*nrow(main_ppdata[N >= 10])/nrow(main_ppdata))`%.

However this does not take account of the clustering of property transactions. 
What proportion of property transactions occur in groups with 10 or more 
observations?

```{r}
main_ppdata[, 
            .(total_transactions = sum(N)), 
            by = .(N >= 10)]
```

This is more promising: only 
`r round(100*sum(main_ppdata[N < 10]$N)/sum(main_ppdata$N))`% of property 
transactions in the tax years 1995-2017 took place in groups with fewer than 10 
data points. We can test this with a higher threshold for 'good data' of n = 20.

```{r}
main_ppdata[, 
            .(total_transactions = sum(N)), 
            by = .(N >= 20)]
```

`r round(100*sum(main_ppdata[N < 20]$N)/sum(main_ppdata$N))`% of transactions 
are in groups that fall below this increased threshold. We can also check 
whether this remains consistent year-on-year.

```{r}
(data_totals <- main_ppdata[, 
                            .(total_transactions = sum(N)), 
                            by = tax_year])

(good_data_totals <- main_ppdata[, 
                                good_data := (N >= 20)][
                                    , 
                                    .(transactions = sum(N)), 
                                    by = .(tax_year, good_data)
                                    ][
                                        good_data == TRUE
                                        ])
setkey(good_data_totals, tax_year)
setkey(data_totals, tax_year)

data_totals[good_data_totals][, 
                              .(good_data_pct = 100 * (transactions / total_transactions)), 
                              by = tax_year] %>% 
    ggplot(., aes(tax_year, good_data_pct)) +
    geom_bar(stat = 'identity', fill = "#008985") +
    coord_cartesian(ylim = c(50, 100)) +
    ggtitle("% of Property Transactions In Groups With Good Data") +
    labs(x = "Tax Year", y = "% of Transactions", 
         caption = "NB. Good data defined as those groups with 20 or more data points.") +
    theme_ipsum_rc()

```

This implies that the proportions are fairly consistent over time, and are above 
`r round(min(data_totals[good_data_totals][, .(good_data_pct = 100 * (transactions / total_transactions)), by = tax_year]$good_data_pct))`% in all years with all but two years above `r round(data_totals[good_data_totals][, .(good_data_pct = 100 * (transactions / total_transactions)), by = tax_year][order(good_data_pct)]$good_data_pct[[3]])`%.

### Trends over time

We need to consider whether the within-year trends seem to be consistent 
between years. Remember that we are considering three levels in our grouping:

- tax year;
- property type;
- area (i.e. postal outcode).

We can examine each of these individually, and then combine them and consider 
overall effects.

#### Year On Year Trends

```{r}
library(scales)
library(viridis)
melt(full_data[, 
          .("Median Price" = quantile(price, .5), 
            "Mean Price" = round(mean(price, na.rm = TRUE))), 
          by = tax_year], 
     id.vars = "tax_year", 
     measure_vars = c("med_price", "avg_price")) %>% 
    ggplot(., aes(x = tax_year, y = value, colour = variable)) +
    geom_line() +
    theme_ipsum_rc() +
    scale_y_comma() +
    theme(legend.title = element_blank()) +
    ggtitle("Property Price Trends 1995-2017") +
    labs(x = "Tax Year", y = "Value in Â£") +
    scale_colour_manual(values = c("#008985", "#0000a0"))

```
Property prices are broadly increasing over time, with mean and median prices 
diverging. This indicates that the data is increasingly right-skewed, which 
tallies with [anecdotal evidence][1] about prices of property in 
London over recent years. (Although [recent articles][2] suggest that house 
prices in London are remaining static or shrinking in some areas, this is not 
yet reflected in the data.)

There are two useful elements to include in the output from this dataset.

1. An indicative price for any property transaction based on type, outcode, 
and tax year. (This will need to be considered along with measures of 
variance to place the expected price in a band.)
2. A measure of the year-to-year change in average price for each type, in 
each outcode. Where we have a previous transaction price for a given property 
this will allow us to compare the empirical and expected change in value: a 
large difference between these may be a useful risk indicator.

## Creating the Output Product

```{r}
output_table <- main_ppdata[, 
                            `:=` (diff_01 = avg_price / lag(avg_price, 01L), 
                                  diff_02 = avg_price / lag(avg_price, 02L), 
                                  diff_03 = avg_price / lag(avg_price, 03L), 
                                  diff_04 = avg_price / lag(avg_price, 04L), 
                                  diff_05 = avg_price / lag(avg_price, 05L), 
                                  diff_06 = avg_price / lag(avg_price, 06L), 
                                  diff_07 = avg_price / lag(avg_price, 07L), 
                                  diff_08 = avg_price / lag(avg_price, 08L), 
                                  diff_09 = avg_price / lag(avg_price, 09L), 
                                  diff_10 = avg_price / lag(avg_price, 10L), 
                                  diff_11 = avg_price / lag(avg_price, 11L), 
                                  diff_12 = avg_price / lag(avg_price, 12L), 
                                  diff_13 = avg_price / lag(avg_price, 13L), 
                                  diff_14 = avg_price / lag(avg_price, 14L), 
                                  diff_15 = avg_price / lag(avg_price, 15L), 
                                  diff_16 = avg_price / lag(avg_price, 16L), 
                                  diff_17 = avg_price / lag(avg_price, 17L), 
                                  diff_18 = avg_price / lag(avg_price, 18L), 
                                  diff_19 = avg_price / lag(avg_price, 19L), 
                                  diff_20 = avg_price / lag(avg_price, 20L), 
                                  diff_21 = avg_price / lag(avg_price, 21L), 
                                  diff_22 = avg_price / lag(avg_price, 22L), 
                                  diff_23 = avg_price / lag(avg_price, 23L)), 
                            by = .(outcde, prop_typ)]

```

Create the output files.

```{r}
write_excel_csv(output_table, "outputs/output_table_xlformat.csv")
write_delim(output_table, "outputs/output_table_pipe.txt")
write_csv(output_table, "outputs/output_table.csv")
write_rds(output_table, "data/output_table.rds")
```


[1]: http://www.homesandproperty.co.uk/property-news/buying/london-house-prices-2015s-top-ten-fastest-rising-boroughs-a93736.html
[2]: https://www.theguardian.com/business/2017/sep/26/london-luxury-property-prices-brexit-savills-market-estate-agent